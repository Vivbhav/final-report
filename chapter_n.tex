%_____________________________________________________________________________________________ 
% LATEX Template: Department of Comp/IT BTech Project Reports
% Sample Chapter
% Sun Mar 27 10:25:35 IST 2011
%
% Note: Itemization, enumeration and other things not shown. A sample table is included.
%_____________________________________________________________________________________________ 

\chapter{Literature Survey}
\section{Automatic Cloze-Question Generation}
This paper describes a system that can generate a list of cloze questions from a
document. CQG system is divided into three main module - Sentence selection, Key
selection and Distractor selection. In the first stage, informative and relevant
sentences are selected from the document. In the second stage, keywords i.e the
words/phrases to be questioned on, are identified in the selected sentence. Key
selection will not be noun or adjective it would find on the basis of NER. The
first two stage are not domain specific. Distractors or answer alternatives for
the keyword in the question sentence are chosen in the final stage using web
scraping. third and final stage is made domain specific, as quality of
distractor depends on domain. 

\section{Automatic question generation on the basis of the discourse
connectives}

This paper describes a question generation system divided into two modules -
Content selection and Question formation. Content selection consists of finding
the relevant part in text to frame question form. The Question formation
involves several tasks like Sense disambiguation of the discourse connectives,
Identification of question type and Applying syntactic transformations on the
content. The authors have concentrated on seven discourse connectives like
because, since, although, as a result, for example and for instance. On that
basis the Question type will be decided using a rule based model. 

\section{Automatic Question Generation Using Software Agents for Technical
Institutions}

This paper explains a system which takes as input a text file and the output is
another text file containing questions. The system is based on Bloom’s taxonomy.
Bloom’s Taxonomy is a set of three hierarchical models used to classify
educational learning objectives into levels of complexity and specificity. The
three lists cover the learning objectives in cognitive, affective and sensory
domains. It has been proved that generating questions based on Bloom's taxonomy
accurately allows a teacher to judge the learning ability of the students. 

The proposed framework helps in question generation by deploying agents, which
perform various operations like Document Processing, Information Classification
and Question Generation. In Document processing phase stemming is carried out to
normalize the senses of the words. Information classification takes an list of
keyword generated by Document Processing and finds the Bloom's category of those
words, by searching appropriate action verb in the repository which fits with
the given keyword. Finally the Question generation module takes the output of
Information classification as input to generate questions. The process is a
template based approach, which fits the selected keywords in the question
template according to the Bloom's levels.

\section{Automatic Multiple Choice Question Generation System for Semantic
Attributes Using String Similarity Measures}

The paper describes a system which selects the informative sentence and the
keyword to generate a question on, based on the semantic labels and named
entities that exist in the sentence. The distractors are chosen based on a
similarity measure between sentences in the data set. For generating question
Semantic Role Labeler and Named Entity Recognizer is used to identify whether
its Name, Location or Name of Organization. Once a question is prepared, then it
measures the similarity between the ‘Question sentence’ and each of the
sentences from the Question knowledge. Then the obtained similarity values from
other sentences are sorted and three keywords from three different sentences are
obtained as distractor values.


\section{G-Asks: An Intelligent Automatic Question Generation System for
Academic Writing Support}

This paper describes a system generating specific trigger questions as a form of
support for student’s learning through writing. It describes a large-scale case
study, of 24 human supervisors and 33 research students, in an Engineering
Research Method course. It compares the questions generated by G-Asks with human
generated questions. The paper analyzes the most frequent question types,
derived from the human supervisors’ questions and discusses how the human
supervisors generate such questions from the source text.

\section{Semantic Based Automatic Question Generation}

This paper also describes a system that uses both Semantic Role Labeling and
Named Entity Recognizer technique to convert the inputted sentence to semantic
pattern. It has developed an Artificial immune system which classifies the
patterns according to the question type. The question types considered here are
the set of WH-questions. Immune system utilizes feature extraction, learning,
storage memory, and associative retrieval in order to solve recognition and
classification tasks. Inputted sentence will first parse using NER and SRL
technique, and from NER and SRL identifies whether, sentences contain person
name, location, date. On the basis of this identification, question pattern can
be created, i.e if person name then question pattern would be WHO.  Sentence
pattern for who and question patterns are interpreted as two features vector in
the training set, one vector for each question type.

\section{Automatic Generation Of Multiple Choice Questions From Domain
Ontologies}

This paper presents an approach that is based on domain specific ontologies and
it is independent of lexicons such as WordNet or other linguistic resources. The
model developed creates multiple choice question items using the Semantic Web
standard technology - Ontology Web Language. The proposed approach is
independent of the domain since questions are generated according to specific
ontology-based strategies. Class based, property based, terminology based
strategies were used to generate the questions. Property-based strategies are
described to produce a large number of multiple choice questions but are said to
be very difficult to manipulate syntactically. Class and terminology-based
strategies on the other hand are much easier to handle syntactically but
generate fewer questions for ontologies of the same depth and population.

\section{Mind the Gap: Learning to Choose Gaps for Question Generation}

In this paper the approach taken is that, the problem of generating good
questions is divided into two parts: First, the selection of sentences to ask
about, and Second, the identification of which part of the resulting sentences
the question should address. To achieve the goal of selecting better gap-fill
questions, author has broken down the task into multiple stages like Sentence
selection, Question construction, and eventually Classification and Scoring. For
generating question, features are used i.e. Token count, lexical, syntactic,
semantic and NER feature is used to generate the Gaps fill question.

\begin{center}
	\begin{longtable}{| p{0.6cm} | p{2.3cm} | p{4cm} | p{2cm} | p{5cm} |}
		\hline
		{\textbf{Sr No.}} & {\textbf{Algorithm}} & {\textbf{Methodology}} &
		{\textbf{Type of Question}} & {\textbf{Evaluation of Result}}\\[2ex]
		\hline
		1. &
		Cloze question generation &
		Sentence selection, key selection and distractor selection is
		domain specific and NER feature is used for key selection &
		Cloze &
		Manually Evaluation is done
		\begin{enumerate}[leftmargin=*]
		\item Evaluation of the selected sentence
		\item Evaluation of selected keyword 
		\item Evaluation of selected distractor.
		\end{enumerate}
		\\[1ex]
		\hline

		2. &
		Automatic question generation on the basis of the discourse
		connectives &
		Content selection and Question formation &
		Question generation like Why, when, where, in which &
		Manually evaluated for semantic and syntactic soundness of
		question by two evaluator
		\\[1ex]
		\hline
		3. &
		Automatic Question Generation Using Software Agents for
		Technical Institutions &
		Document Processing, Information Classification and Question
		Generation. &
		Define, Describes, Give example, long descriptive questions &
		-
		\\[1ex]
		\hline
		4. &
		G-Ask &
		Citation Extraction, Citation Classification. and Generation &
		Long descriptive questions like Why, when, Does any.. &
		Compared questions generated by the system to those produced by
		humans. and Citation Classification performance is done through
		precision and recall.
		\\[1ex]
		\hline
		5. &
		Automatic Multiple Choice Question Generation System &
		Extract sentence from Data Set, Prepare Question sentence,
		Measure the similarity between the question sentence and all
		sentences in the knowledge base, Return the three sentences that
		have the highest similarity values, three keywords of three
		sentences as distractor selection &
		MCQ &
		In this research out of nearly 145 parsed sentences, there were
		109 considered good according to the keywords that are extracted
		from them.
		\\[1ex]
		\hline
		6. &
		Semantic Based Automatic Question Generation &
		Input sentence, Feature Extraction through SRL, NER, Choose MCS,
		Test Sentence pattern and Test the Question type pattern &
		WH-questions like who, when, where, why, and how. &
		170 sentences are extracted and mapped into 250 patterns using
		SRL and NER. The 250 patterns are used in training and testing.
		and Precision, Recall and F-measurement is used for
		classification of question type.
		The percentage of truly generated patterns increased 87\% which
		appears to be promising ratio in this problem comparing it to
		other techniques used in generating questions automatically.
		\\[1ex]
		\hline
		7. &
		Automatic Generation of Multiple Choice Questions From Domain
		Ontologies &
		Ontology-based strategies like class based, property based,
		terminology based strategies &
		MCQ (Choose the correct sentence) &
		The generated questionnaires were evaluated in three dimensions:
		Pedagogical quality, linguistic/syntactical correctness and
		number of questions produced.
		\\[1ex]
		\hline
		8. &
		Mind the Gap: Learning to Choose Gaps for Question Generation &
		1)Sentence selection,
		2)Question construction,
		3)Classification/Scoring. &
		Fill in the blanks question &
		manually analyze the generated questions and rate the question
		\\[1ex]
		\hline
	\end{longtable}
\end{center}


%\begin{table}[H]		% Table
%\begin{center}		
%\begin{tabular}{ | c | c | }	% Format	
%\hline
%\multicolumn{2}{|c|}{Table 1: Test Century Records }\\
%\hline
%\bf{Batsman} & \bf{Test Centuries}\\ \hline
%Sachin & 51 \\ \hline
%Kallis & 40 \\ \hline
%Ponting & 39 \\ \hline
%Lara, Gavaskar & 34\\ 
%\hline
%\end{tabular}
%\caption{A simple table: Test centuries}	% This will appear in List of Tables
%\label{table1}
%\end{center}
%\end{table}

%_____________________________________________________________________________________________ 

