%_____________________________________________________________________________________________ 
% LATEX Template: Department of Comp/IT BTech Project Reports
% Sample Chapter
% Sun Mar 27 10:25:35 IST 2011
%
% Note: Itemization, enumeration and other things not shown. A sample table is included.
%_____________________________________________________________________________________________ 

\chapter{Literature Survey}

\begin{enumerate}[align=left]

	\item \textbf{Computational Intelligence Framework for Automatic Quiz
		Question Generation}
		
		The model described in this paper, uses a rule based approach
		and is able to generate three types of questions, namely, Fill
		in the Blank, True/False, and “Wh” Questions. The training phase
		of this algorithm involves, detecting topically important
		sentences, NER and POS Tagging, sanitizing the input sentence
		and then constructing a tree for further processing of each type
		of question.  For Fill in the Blank, it gives weight to each
		word involved and marks important words for setting up blanks.
		Thr true or false sentence involves identifying modal verbs, and
		also factual type of sentences and further rule based
		processing. For “Wh” questions, patterns identified during the
		training phase are matched, and used to generate questions. 

\item \textbf{Automatic Question Generation System}
	
	This is a traditional supervised learning approach and the model can be
		broken down into a sequence of processes.  The text input is
		stemmed to get the meaning of each word and then passed to the
		Phrase Mapper. Then the key phrases are extracted using pre
		trained documents and the document is summarized. In the end,
		Nouns are filtered in, on which questions can be generated. 

\item \textbf{Automatic Question Generation for Intelligent Tutoring Systems}
	
	This system generates only Multiple Choice Questions, and has been
		trained on Wikipedia articles. The wikipedia articles are
		processed and unigrams, bigrams are extracted. The keywords from
		them are then extracted and their weights using the TF IDF
		weighting scheme are calculated. Using the above distractors can
		be generated. Whenever a query is fired to generate questions
		on, the wikipedia articles of the keyword is taken up, and rule
		based methods are applied to the sentences to create questions
		and presented to the users including the distractors as other
		viable options. 

\item \textbf{Automatic Question Generation from Children’s Stories for
	Companion Chatbot}
		
This paper was specifically targeted for generating questions from children’s
		stories, and was trained and tested on much lesser data than
		others. The model is said to work in two parts, that is one for
		question generation and the other for ranking of the questions.
		The model uses the part of speech tags and dependency parsing
		algorithms along with pre trained language rules for the
		question generation phase. For ranking of the questions, the
		model uses logistic regression to determine the acceptability
		probability of the question. 

\item \textbf{Deep Guessing}
	
	Generating Meaningful Personalized Quizzes on Historical Topics by
		Introducing Wikicategories in Doc2Vec: The aim of this model is
		to load all of the wikipedia articles, classify them into
		categories, and use the knowledge from the above, for generation
		of distractors in multiple choice questions. The model works on
		a novel idea of creation of paragraph vectors, whose advantage
		is that they are trained from unlabeled data and thus can work
		well for tasks which do not have enough labelled datasets. After
		the score of wiki categories is obtained, thresholds are decided
		to classify the records into categories and thus, are eventually
		used for the creation of meaningful options in the questions
		generated. 

\item \textbf{Automatic question generation on the basis of the discourse
	connectives}
	
	This problem of question generation has been divided into two modules in
	this paper.  The first part is that of Content selection and the next
	that of Question formation. The Content selection phase consists of
	recognizing the part in text that is relevant and important to generate
	questions on. The second phase of Question formation includes several
	subtasks like Word Sense disambiguation of the discourse connectives
	then the Identification of the type of question to be created and
	eventually applying syntactic transformations on the context. The paper
	mainly takes into consideration the seven main discourse connectives -
	although, as a result, because, for example for instance and since.
	Using the output generated so far, the type of question gets decided and
	then the question can be formed. 

\item \textbf{Semantic Based Automatic Question Generation}
	
	The paper explains a system that applies two fundamental Natural
	Language Processing concepts namely Semantic Role Labeling and Named
	Entity Recognizer technique. These tasks are used to convert the
	inputted sentence to semantic pattern. The model described in the paper
	has developed a system which has identified patterns for each type of
	question. The question types under consideration here are all of the
	‘Wh’-questions - who, when, why, where, what.  The system for
	classification uses learning, storage memory, feature extraction, and
	associative Retrieval.  

The sentence given as input will be first parsed using Named Entity Recognition
and SRL technique. The output of these two algorithms has a direct correlation
with the exact question type to be created. Thus after the question type
identification, the question pattern is known using the pretrained rules. 

\item \textbf{Automatic Multiple Choice Question Generation System for Semantic
	Attributes Using String Similarity Measures}
	
	The paper has described a model which first selects a factual sentence
	and an important word to generate questions on from the text given as
	input. This selection is done on the basis of the semantic labels and
	named entities in the sentence. Then for actually generating a question
	the SRL and NER tag is used, which directly helps in finding out the
	type of question. 

Then the model focuses on finding distractors i.e. the incorrect options of a
question. For this task  different similarity measure between sentences of the
data set is taken into account. Eventually, when a question is generated, the
measures of similarity between the actual question and the sentences of the
input text is considered and sorted. The top three sentences obtained from the
above procedure, are considered for finding a relevant important word to the
question generated which are eventually used as distractors. 

\end{enumerate}

\begin{center}
	\begin{longtable}{| p{0.6cm} | p{2.3cm} | p{4cm} | p{2cm} | p{5cm} |}
		\caption{Comparison Table}\\
		\hline
		{\textbf{Sr No.}} & {\textbf{Algorithm}} & {\textbf{Methodology}} &
		{\textbf{Type of Question}} & {\textbf{Evaluation of Result}}\\[2ex]
		\hline
		1. &
		Cloze question generation &
		Sentence selection, key selection and distractor selection is
		domain specific and NER feature is used for key selection &
		Cloze &
		Manually Evaluation is done
		\begin{enumerate}
		\item Evaluation of the selected sentence
		\item Evaluation of selected keyword 
		\item Evaluation of selected distractor.
		\end{enumerate}
		\\[1ex]
		\hline

		2. &
		Automatic question generation on the basis of the discourse
		connectives &
		Content selection and Question formation &
		Question generation like Why, when, where, in which &
		Manually evaluated for semantic and syntactic soundness of
		question by two evaluator
		\\[1ex]
		\hline
		3. &
		Automatic Question Generation Using Software Agents for
		Technical Institutions &
		Document Processing, Information Classification and Question
		Generation. &
		Define, Describes, Give example, long descriptive questions &
		-
		\\[1ex]
		\hline
		4. &
		G-Ask &
		Citation Extraction, Citation Classification. and Generation &
		Long descriptive questions like Why, when, Does any.. &
		Compared questions generated by the system to those produced by
		humans. and Citation Classification performance is done through
		precision and recall.
		\\[1ex]
		\hline
		5. &
		Automatic Multiple Choice Question Generation System &
		Extract sentence from Data Set, Prepare Question sentence,
		Measure the similarity between the question sentence and all
		sentences in the knowledge base, Return the three sentences that
		have the highest similarity values, three keywords of three
		sentences as distractor selection &
		MCQ &
		In this research out of nearly 145 parsed sentences, there were
		109 considered good according to the keywords that are extracted
		from them.
		\\[1ex]
		\hline
		6. &
		Semantic Based Automatic Question Generation &
		Input sentence, Feature Extraction through SRL, NER, Choose MCS,
		Test Sentence pattern and Test the Question type pattern &
		WH-questions like who, when, where, why, and how. &
		170 sentences are extracted and mapped into 250 patterns using
		SRL and NER. The 250 patterns are used in training and testing.
		and Precision, Recall and F-measurement is used for
		classification of question type.
		The percentage of truly generated patterns increased 87\% which
		appears to be promising ratio in this problem comparing it to
		other techniques used in generating questions automatically.
		\\[1ex]
		\hline
		7. &
		Automatic Generation of Multiple Choice Questions From Domain
		Ontologies &
		Ontology-based strategies like class based, property based,
		terminology based strategies &
		MCQ (Choose the correct sentence) &
		The generated questionnaires were evaluated in three dimensions:
		Pedagogical quality, linguistic/syntactical correctness and
		number of questions produced.
		\\[1ex]
		\hline
		8. &
		Mind the Gap: Learning to Choose Gaps for Question Generation &
		1)Sentence selection,
		2)Question construction,
		3)Classification/Scoring. &
		Fill in the blanks question &
		manually analyze the generated questions and rate the question
		\\[1ex]
		\hline
	\end{longtable}
\end{center}


%\begin{table}[H]		% Table
%\begin{center}		
%\begin{tabular}{ | c | c | }	% Format	
%\hline
%\multicolumn{2}{|c|}{Table 1: Test Century Records }\\
%\hline
%\bf{Batsman} & \bf{Test Centuries}\\ \hline
%Sachin & 51 \\ \hline
%Kallis & 40 \\ \hline
%Ponting & 39 \\ \hline
%Lara, Gavaskar & 34\\ 
%\hline
%\end{tabular}
%\caption{A simple table: Test centuries}	% This will appear in List of Tables
%\label{table1}
%\end{center}
%\end{table}

%_____________________________________________________________________________________________ 

