%_____________________________________________________________________________________________ 
% LATEX Template: Department of Comp/IT BTech Project Reports
% Sample Chapter
% Sun Mar 27 10:25:35 IST 2011
%
% Note: Itemization, enumeration and other things not shown. A sample table is included.
%_____________________________________________________________________________________________ 

\chapter{Literature Survey}

\begin{enumerate}[align=left]

	\item \textbf{Computational Intelligence Framework for Automatic Quiz
		Question Generation}
		
		The model described in this paper, uses a rule based approach
		and is able to generate three types of questions, namely, Fill
		in the Blank, True/False, and “Wh” Questions. The training phase
		of this algorithm involves, detecting topically important
		sentences, NER and POS Tagging, sanitizing the input sentence
		and then constructing a tree for further processing of each type
		of question.  For Fill in the Blank, it gives weight to each
		word involved and marks important words for setting up blanks.
		Thr true or false sentence involves identifying modal verbs, and
		also factual type of sentences and further rule based
		processing. For “Wh” questions, patterns identified during the
		training phase are matched, and used to generate questions. 

\item \textbf{Automatic Question Generation System}
	
	This is a traditional supervised learning approach and the model can be
		broken down into a sequence of processes.  The text input is
		stemmed to get the meaning of each word and then passed to the
		Phrase Mapper. Then the key phrases are extracted using pre
		trained documents and the document is summarized. In the end,
		Nouns are filtered in, on which questions can be generated. 

\item \textbf{Automatic Question Generation for Intelligent Tutoring Systems}
	
	This system generates only Multiple Choice Questions, and has been
		trained on Wikipedia articles. The wikipedia articles are
		processed and unigrams, bigrams are extracted. The keywords from
		them are then extracted and their weights using the TF IDF
		weighting scheme are calculated. Using the above distractors can
		be generated. Whenever a query is fired to generate questions
		on, the wikipedia articles of the keyword is taken up, and rule
		based methods are applied to the sentences to create questions
		and presented to the users including the distractors as other
		viable options. 

\item \textbf{Automatic Question Generation from Children’s Stories for
	Companion Chatbot}
		
This paper was specifically targeted for generating questions from children’s
		stories, and was trained and tested on much lesser data than
		others. The model is said to work in two parts, that is one for
		question generation and the other for ranking of the questions.
		The model uses the part of speech tags and dependency parsing
		algorithms along with pre trained language rules for the
		question generation phase. For ranking of the questions, the
		model uses logistic regression to determine the acceptability
		probability of the question. 

\item \textbf{Deep Guessing}
	
	Generating Meaningful Personalized Quizzes on Historical Topics by
		Introducing Wikicategories in Doc2Vec: The aim of this model is
		to load all of the wikipedia articles, classify them into
		categories, and use the knowledge from the above, for generation
		of distractors in multiple choice questions. The model works on
		a novel idea of creation of paragraph vectors, whose advantage
		is that they are trained from unlabeled data and thus can work
		well for tasks which do not have enough labelled datasets. After
		the score of wiki categories is obtained, thresholds are decided
		to classify the records into categories and thus, are eventually
		used for the creation of meaningful options in the questions
		generated. 

\item \textbf{Automatic question generation on the basis of the discourse
	connectives}
	
	This problem of question generation has been divided into two modules in
	this paper.  The first part is that of Content selection and the next
	that of Question formation. The Content selection phase consists of
	recognizing the part in text that is relevant and important to generate
	questions on. The second phase of Question formation includes several
	subtasks like Word Sense disambiguation of the discourse connectives
	then the Identification of the type of question to be created and
	eventually applying syntactic transformations on the context. The paper
	mainly takes into consideration the seven main discourse connectives -
	although, as a result, because, for example for instance and since.
	Using the output generated so far, the type of question gets decided and
	then the question can be formed. 

\item \textbf{Semantic Based Automatic Question Generation}
	
	The paper explains a system that applies two fundamental Natural
	Language Processing concepts namely Semantic Role Labeling and Named
	Entity Recognizer technique. These tasks are used to convert the
	inputted sentence to semantic pattern. The model described in the paper
	has developed a system which has identified patterns for each type of
	question. The question types under consideration here are all of the
	‘Wh’-questions - who, when, why, where, what.  The system for
	classification uses learning, storage memory, feature extraction, and
	associative Retrieval.  

The sentence given as input will be first parsed using Named Entity Recognition
and SRL technique. The output of these two algorithms has a direct correlation
with the exact question type to be created. Thus after the question type
identification, the question pattern is known using the pretrained rules. 

\item \textbf{Automatic Multiple Choice Question Generation System for Semantic
	Attributes Using String Similarity Measures}
	
	The paper has described a model which first selects a factual sentence
	and an important word to generate questions on from the text given as
	input. This selection is done on the basis of the semantic labels and
	named entities in the sentence. Then for actually generating a question
	the SRL and NER tag is used, which directly helps in finding out the
	type of question. 

Then the model focuses on finding distractors i.e. the incorrect options of a
question. For this task  different similarity measure between sentences of the
data set is taken into account. Eventually, when a question is generated, the
measures of similarity between the actual question and the sentences of the
input text is considered and sorted. The top three sentences obtained from the
above procedure, are considered for finding a relevant important word to the
question generated which are eventually used as distractors. 

\end{enumerate}

\begin{center}
	\begin{longtable}{| p{0.6cm} | p{2.3cm} | p{4cm} | p{2cm} | p{5cm} |}
		\caption{Comparison Table}\\
		\hline
		{\textbf{Sr No.}} & {\textbf{Algorithm}} & {\textbf{Methodology}} &
		{\textbf{Type of Question}} & {\textbf{Evaluation of Result}}\\[2ex]
		\hline
		1. &
		Computationally Intelligent Techniques &
		Rule Based Approach - Training for patterns and then generate &
		Fill in the Blanks, True/False, “Wh” questions & 
		Manual checking for finding difference between Automatic Questions and Domain Experts - users found difference 
		\\[1ex]
		\hline

		2. &
		Automatic Question Generation System &
		Series of pre processes like stemming, summarize, noun extract, and then rule based question generation &
		All “Wh” questions along with type like - Person, Definition, Procedure, Consequence, etc &
		Compression and Omission Ration considered - acceptable results
		\\[1ex]
		\hline
		3. &
		AQG for Intelligent Tutoring Systems &
		Keywords from Wikipedia and then TF-IDF, then rule based &
		MCQ’s alongwith distractors &
		Manual evaluation with domain expert
		\\[1ex]
		\hline
		4. &
		AQG from Children’s Stories for Companion Chatbot  &
		POS Tagger, Dependency Parsing and Logistic Regression &
		Only “Wh” questions &
		Manual evaluation with over 50\% acceptance		
		\\[1ex]
		\hline
		5. &
		Deep Guessing: Quizzes on Historical Topics by Wikicategories in Doc2Vec:  &
		Generating paragraph vectors from wikipedia articles, categorization for distractors &
		MCQ Questions with distractors &
		Manual evaluation of 45 MCQ’s \\[1ex]
		\hline
		6. &
		AQG on the basis of the discourse connectives &
		Content selection and Question formation &
		Question generation like Why, when, where, in which &
		Manually evaluated for semantic and syntactic soundness of question by two evaluator 
		\\[1ex]
		\hline
		7. &
		Semantic based AQG &
		Feature Extraction using NER, SRL, Sentence pattern finding and  get question type pattern &
		Only “Wh” questions &
		Manual evaluation with small dataset, About 87\% accuracy		
		\\[1ex]
		\hline
		8. &
		AQG System for Semantic Attributes Using String Similarity Measures &
		Extracting sentence, check viability by similarity of sentences and select top three sentences &
		Only “MCQ” type questions &
		Manual evaluation with 75\% success. 
		\\[1ex]
		\hline
	\end{longtable}
\end{center}


%\begin{table}[H]		% Table
%\begin{center}		
%\begin{tabular}{ | c | c | }	% Format	
%\hline
%\multicolumn{2}{|c|}{Table 1: Test Century Records }\\
%\hline
%\bf{Batsman} & \bf{Test Centuries}\\ \hline
%Sachin & 51 \\ \hline
%Kallis & 40 \\ \hline
%Ponting & 39 \\ \hline
%Lara, Gavaskar & 34\\ 
%\hline
%\end{tabular}
%\caption{A simple table: Test centuries}	% This will appear in List of Tables
%\label{table1}
%\end{center}
%\end{table}

%_____________________________________________________________________________________________ 

